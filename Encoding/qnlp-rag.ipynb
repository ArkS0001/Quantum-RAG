{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install lambeq pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fljs4yEtbNpD",
        "outputId": "a5b985b0-5e38-4ea2-a5b2-1ed108c2eb02"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lambeq\n",
            "  Downloading lambeq-0.4.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.11/dist-packages (0.40.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from lambeq) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lambeq) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.1 in /usr/local/lib/python3.11/dist-packages (from lambeq) (11.1.0)\n",
            "Collecting pytket>=1.31.0 (from lambeq)\n",
            "  Downloading pytket-2.0.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from lambeq) (6.0.2)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.11/dist-packages (from lambeq) (3.7.5)\n",
            "Collecting tensornetwork (from lambeq)\n",
            "  Downloading tensornetwork-0.4.6-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.11/dist-packages (from lambeq) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from lambeq) (4.48.3)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.16.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.7.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: pennylane-lightning>=0.40 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.40.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.2->lambeq) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.2->lambeq) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.2->lambeq) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.2->lambeq) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.2->lambeq) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.2->lambeq) (2.8.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning>=0.40->pennylane) (0.3.29.0.0)\n",
            "Requirement already satisfied: sympy>=1.12.1 in /usr/local/lib/python3.11/dist-packages (from pytket>=1.31.0->lambeq) (1.13.1)\n",
            "Collecting lark>=1.1.9 (from pytket>=1.31.0->lambeq)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: graphviz>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from pytket>=1.31.0->lambeq) (0.20.3)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from pytket>=1.31.0->lambeq) (3.1.5)\n",
            "Collecting qwasm>=1.0.1 (from pytket>=1.31.0->lambeq)\n",
            "  Downloading qwasm-1.0.1-py3-none-any.whl.metadata (299 bytes)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (2.10.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->lambeq) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->lambeq) (3.17.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->lambeq) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->lambeq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->lambeq) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->lambeq) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.12.1->pytket>=1.31.0->lambeq) (1.3.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from tensornetwork->lambeq) (3.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensornetwork->lambeq) (3.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->lambeq) (0.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->lambeq) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->lambeq) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->lambeq) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.4->pytket>=1.31.0->lambeq) (3.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0->lambeq) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->lambeq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->lambeq) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.2->lambeq) (1.17.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0->lambeq) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0->lambeq) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->lambeq) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->lambeq) (7.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0->lambeq) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0->lambeq) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (0.1.2)\n",
            "Downloading lambeq-0.4.3-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytket-2.0.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m771.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensornetwork-0.4.6-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.3/364.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qwasm-1.0.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: qwasm, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, tensornetwork, pytket, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lambeq\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lambeq-0.4.3 lark-1.2.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytket-2.0.1 qwasm-1.0.1 tensornetwork-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DisCoCat for Text-to-Quantum Conversion\n",
        "\n",
        "    What is DisCoCat? DisCoCat (Distributional Compositional Categorical) is a framework for representing the meaning of natural language sentences using category theory and quantum mechanics. It aims to combine the distributional properties of words (their statistical relationships with other words) with the compositional structure of sentences (how words combine to form phrases and meanings).\n",
        "\n",
        "    How It Works:\n",
        "\n",
        "        Diagrammatic Representation: DisCoCat represents sentences as string diagrams. These diagrams show how words and their meanings combine according to the grammatical structure of the sentence.\n",
        "\n",
        "        Compact Tensor Representation: Each word has an associated vector, similar to Word2Vec. The way those vectors are connected by the diagram to form a complete sentence is represented using Tensor algebra.\n",
        "\n",
        "        Quantum Analog: Each word is converted to a \"quantum\" state.\n",
        "\n",
        "    Benefits:\n",
        "\n",
        "        Compositionality: Explicitly captures how the meaning of a sentence arises from the combination of its constituent words.\n",
        "\n",
        "        Mathematical Foundation: Provides a strong mathematical framework for reasoning about language.\n",
        "\n",
        "        Potential for Quantum Implementation: The tensor representation lends itself to potential implementation on quantum computers.\n",
        "\n",
        "    Implementation:\n",
        "\n",
        "        lambeq: The primary toolkit to implement the steps for converting text to quantum, and you may want to work with it more closely.\n",
        "\n",
        "II. PennyLane for Encoding\n",
        "\n",
        "    Role: PennyLane is an excellent choice for the quantum parts, by converting the tensor to circuits.\n",
        "\n",
        "    How it would connect to DisCoCat: PennyLane should convert the vectors from DisCoCat.\n",
        "\n",
        "    Encoding Choice\n",
        "\n",
        "        Amplitude Encoding: Encoding these states as amplitudes of the state.\n",
        "\n",
        "        Feature Maps: Using a feature map to represent these vectors, so you will be making good use of the VQC circuit in Pennylane. This is a flexible step to perform circuit optimization.\n",
        "\n",
        "III. Grover's Search for Retrieval\n",
        "\n",
        "    Approach: Use Grover's algorithm to search for relevant quantum documents that are similar to the quantum query.\n",
        "\n",
        "    Critical Implementation Points:\n",
        "\n",
        "        Oracle Design:\n",
        "\n",
        "            Threshold or Learned Oracle: Using a threshold oracle requires a way to determine whether a document is relevant (which is hard to classically extract), or using a VQC to determine how well the retrieval is actually happening.\n",
        "\n",
        "    Assumptions:\n",
        "\n",
        "        For QRAM - this is difficult to resolve so you have to work with simpler methods.\n",
        "\n",
        "        For the number of Grover iterations, should be determined given the quality of the oracle.\n",
        "\n",
        "IV. Ranking\n",
        "\n",
        "    Purpose: After Grover's search, you'll have a set of potentially relevant documents. Ranking helps you order these documents based on their relevance to the query.\n",
        "\n",
        "    Approaches:\n",
        "\n",
        "        Quantum-Assisted Ranking: If possible, use a quantum algorithm to refine the ranking.\n",
        "\n",
        "            Could involve a second, more precise similarity estimation step using QAE.\n",
        "\n",
        "            Could train a VQC to learn a quantum ranking function.\n",
        "\n",
        "    Classical fallback: If it is too much effort, just return the top vector from Grover.\n",
        "\n",
        "V. Workflow\n",
        "\n",
        "    Text Input: Start with a text corpus and a query.\n",
        "\n",
        "    DisCoCat Parsing: Use DisCoCat to parse the query and each document in the corpus, converting them into string diagrams and then into quantum states (density matrices).\n",
        "\n",
        "    PennyLane Encoding: Use PennyLane to encode the DisCoCat-generated quantum states into quantum circuits. Choose an appropriate encoding method (amplitude encoding, angle encoding, or feature maps) based on the nature of the DisCoCat output and the available quantum hardware.\n",
        "\n",
        "    Quantum Indexing: Store the quantum states of the documents in a quantum index (e.g., QuAM, or a simpler simulated index for now).\n",
        "\n",
        "    Grover's Search: Use Grover's algorithm to search the quantum index for documents that are similar to the quantum query.\n",
        "\n",
        "    Ranking: Rank the retrieved documents based on their relevance to the query (using quantum-assisted ranking or a classical ranking function).\n",
        "\n",
        "    Output: Return the ranked list of documents as the result.\n",
        "\n",
        "VI. Potential Challenges and Mitigation Strategies\n",
        "\n",
        "    Scalability:\n",
        "\n",
        "        Challenge: DisCoCat can generate complex string diagrams, which can lead to large quantum circuits that are difficult to simulate or implement on near-term quantum hardware. Amplitude encoding also has scalability limits.\n",
        "\n",
        "        Mitigation:\n",
        "\n",
        "            Circuit Simplification: Develop techniques for simplifying the DisCoCat-generated string diagrams before converting them into quantum circuits.\n",
        "\n",
        "            Feature Selection: Select the most important features from the DisCoCat output to reduce the size of the quantum circuits.\n",
        "\n",
        "            Modular Design: Break down the quantum circuits into smaller, modular components that can be executed separately.\n",
        "\n",
        "            Sparse QRAM (If Available): If truly available, use efficient storage mechanisms to reduce space complexity.\n",
        "\n",
        "    Expressivity:\n",
        "\n",
        "        Challenge: DisCoCat may not capture all the nuances of natural language meaning.\n",
        "\n",
        "        Mitigation:\n",
        "\n",
        "            Hybrid Approach: Combine DisCoCat with other NLP techniques (e.g., transformers) to enrich the semantic representation.\n",
        "\n",
        "            Knowledge Integration: Incorporate external knowledge sources (e.g., ontologies, knowledge graphs) into the DisCoCat framework.\n",
        "\n",
        "    Hardware Limitations:\n",
        "\n",
        "        Challenge: Current quantum hardware has limited qubit counts, coherence times, and gate fidelities.\n",
        "\n",
        "        Mitigation:\n",
        "\n",
        "            Hardware-Aware Circuit Design: Design the quantum circuits to match the specific architecture and connectivity of the available quantum hardware.\n",
        "\n",
        "            Error Mitigation: Implement error mitigation techniques to reduce the effects of quantum noise.\n",
        "\n",
        "VII. Implementation Steps (Conceptual)\n",
        "\n",
        "    Set Up Your Environment: Install PennyLane, lambeq, and other necessary libraries.\n",
        "\n",
        "    Data Preparation: Pre-process your text corpus.\n",
        "\n",
        "    DisCoCat Conversion: Use DisCoCat to convert the corpus and query into string diagrams and then into tensor representations.\n",
        "\n",
        "    PennyLane Encoding: Implement quantum circuits in PennyLane to encode the DisCoCat-generated tensor representations.\n",
        "\n",
        "    Simulated Quantum Index: For now, create a classical data structure to simulate the quantum index (since QuAM is not yet available).\n",
        "\n",
        "    Grover's Search: Implement Grover's algorithm in PennyLane to search the simulated quantum index.\n",
        "\n",
        "    Ranking: Implement a classical ranking function to rank the retrieved documents.\n",
        "\n",
        "    Evaluation: Evaluate the performance of your QNLP-RAG system using appropriate metrics (e.g., precision, recall, F1-score)."
      ],
      "metadata": {
        "id": "M4EmqIfHa7To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import pennylane as qml\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import requests, zipfile\n",
        "from shutil import copyfileobj\n",
        "import lambeq as lb\n",
        "\n",
        "# ---------------------- Section I: Setup ----------------------\n",
        "# 1. Download necessary NLTK resources\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "try:\n",
        "    word_tokenize(\"example text\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# ---------------------- Section II: Load Resources and Preprocessing ----------------------\n",
        "# Load Text Corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"A dog is a loyal companion.\",\n",
        "    \"The fox is a cunning animal.\",\n",
        "    \"Quantum computing is promising.\",\n",
        "    \"NLP is a branch of AI.\",\n",
        "    \"Earth is the third planet.\",\n",
        "    \"AI is transforming industries.\",\n",
        "    \"Climate change is a global issue.\",\n",
        "    \"Renewable energy is important.\"\n",
        "]\n",
        "\n",
        "# Get user query\n",
        "question = input(\"Enter your query: \")\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    return tokens\n",
        "\n",
        "processed_corpus = [preprocess(doc) for doc in corpus]\n",
        "processed_question = preprocess(question)\n",
        "\n",
        "# ---------------------- Section III: DisCoCat to Quantum Conversion ----------------------\n",
        "# Initialize DisCoCat model\n",
        "grammar = lb.BobcatParse()\n",
        "\n",
        "# Create circuit using DisCoCat parser\n",
        "def get_quantum_state(text):\n",
        "      diagram = grammar.parse(text)\n",
        "      return diagram\n",
        "\n",
        "diagram_corpus = [get_quantum_state(doc) for doc in corpus]\n",
        "diagram_query = get_quantum_state(question)\n",
        "\n",
        "# ---------------------- Section IV: PennyLane Encoding ----------------------\n",
        "num_qubits = 10  # Example, adjust based on needs\n",
        "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def quantum_encoding_circuit(discocat_tensor_rep, params):\n",
        "    \"\"\"Variational Quantum Circuit to map DisCoCat tensor representation to Quantum State.\"\"\"\n",
        "    qml.Hadamard(wires=0)\n",
        "    qml.CNOT(wires=[0, 1])  # Entanglement\n",
        "    # Apply some sort of encoding to use the data as weights for the rotation\n",
        "    return qml.state() #Returns the Quantum State\n",
        "\n",
        "quantum_corpus_states = [quantum_encoding_circuit(diagram_i, params) for diagram_i in diagram_corpus] # Quantum States of Corpus\n",
        "quantum_query_state = quantum_encoding_circuit(diagram_query, params) # Quantum State of Query\n",
        "\n",
        "# ---------------------- Section V: Quantum Retrieval (Grover's Algorithm) ----------------------\n",
        "num_document_qubits = int(np.ceil(np.log2(len(corpus))))\n",
        "dev_grover = qml.device(\"default.qubit\", wires=num_document_qubits) # Quantum Device for Grover's alg\n",
        "# Simulated Quantum Similarity Calculation (Replace with QAE later)\n",
        "def quantum_state_similarity(state1, state2): #Simulated Quantum Similarity\n",
        "  similarity = cosine_similarity(np.real(state1).reshape(1,-1), np.real(state2).reshape(1,-1))[0][0]\n",
        "  return similarity\n",
        "\n",
        "@qml.qnode(dev_grover)\n",
        "def grover_search(quantum_states, quantum_query):\n",
        "\n",
        "  #Oracle Phase\n",
        "  def oracle(wires):\n",
        "        max_sim = max([quantum_state_similarity(quantum_query, quantum_states[i]) for i in range(len(quantum_states))])\n",
        "        similarities = [quantum_state_similarity(quantum_query, quantum_states[i]) for i in range(len(quantum_states))]\n",
        "        for i in range(len(quantum_states)):\n",
        "            if similarities[i] >= max_sim:\n",
        "                qml.FlipSign(wires=wires, n = 1)\n",
        "\n",
        "  def grover_diffusion_op(wires):\n",
        "    \"\"\"Grover diffusion operator.\"\"\"\n",
        "    num_document_qubits = int(np.ceil(np.log2(len(corpus))))\n",
        "    for wire in wires:\n",
        "        qml.Hadamard(wires=wire)\n",
        "    for wire in wires:\n",
        "        qml.PauliX(wires=wire)\n",
        "    if len(wires) > 1:  # Apply CZ only if there are at least two qubits\n",
        "        qml.CZ(wires=[wires[0], wires[1]])\n",
        "    for wire in wires:\n",
        "        qml.PauliX(wires=wire)\n",
        "    for wire in wires:\n",
        "        qml.Hadamard(wires=wire)\n",
        "\n",
        "  # Apply Hadamards\n",
        "  wires = range(dev_grover.num_wires)\n",
        "  for wire in wires:\n",
        "    qml.Hadamard(wires=wire)\n",
        "\n",
        "    # Number of Grover iterations\n",
        "  N = len(quantum_states)\n",
        "  num_iterations = int(np.floor(np.pi / 4 * np.sqrt(N)))\n",
        "   #Run Grover's\n",
        "  for _ in range(num_iterations):\n",
        "      oracle(wires)\n",
        "      grover_diffusion_op(wires)\n",
        "\n",
        "  return qml.probs(wires=wires) # Output\n",
        "\n",
        "probabilities = grover_search(quantum_corpus_states,quantum_query_state)  # Run Grover and get Probabilities\n",
        "most_likely_index = np.argmax(probabilities)  # Index of most likely document.\n",
        "\n",
        "# ---------------------- Section VI: Quantum Answer Extraction (Simulated) ----------------------\n",
        "# --- Replace with Learned Quantum Answer Extraction Section ---\n",
        "retrieved_document = corpus[most_likely_index]\n",
        "print(f\"Query: {question}\")\n",
        "print(f\"Answer: {retrieved_document}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "dF3BbTu_bBiM",
        "outputId": "1cbc21a4-08b8-4960-e9a7-736388ed6528"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query: NLP\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'lambeq' has no attribute 'BobcatParse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b7c47aebdfdd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# ---------------------- Section III: DisCoCat to Quantum Conversion ----------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Initialize DisCoCat model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBobcatParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Create circuit using DisCoCat parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'lambeq' has no attribute 'BobcatParse'"
          ]
        }
      ]
    }
  ]
}